{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Dim reduction tools\n",
    "import umap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# \n",
    "from utils import *\n",
    "import segypy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy paste \"utils.py\" because it wont load/import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_seismic(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    seis, header, trace_headers = segypy.readSegy(filename)\n",
    "    amplitude = seis.reshape(header['ns'], inl.size, crl.size)\n",
    "    lagtime = trace_headers['LagTimeA'][0]*-1\n",
    "    twt = np.arange(lagtime, header['dt']/1e3*header['ns']+lagtime, header['dt']/1e3)\n",
    "    return amplitude, twt\n",
    "\n",
    "\n",
    "def load_horizon(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    hrz = np.recfromtxt(filename, names=['il','xl','z'])\n",
    "    horizon = np.zeros((len(inl), len(crl)))\n",
    "    for i, idx in enumerate(inl):\n",
    "        for j, xdx in enumerate(crl):\n",
    "            time = hrz['z'][np.where((hrz['il']== idx) & (hrz['xl'] == xdx))]\n",
    "            if len(time) == 1:\n",
    "                horizon[i, j] = time \n",
    "\n",
    "    return horizon\n",
    "\n",
    "def colorbar(mappable):\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    return fig.colorbar(mappable, cax=cax)\n",
    "\n",
    "def interpolate_horizon(horizon):\n",
    "    points = []\n",
    "    wanted = []\n",
    "    for i in range(horizon.shape[0]):\n",
    "        for j in range(horizon.shape[1]):\n",
    "            if horizon[i, j] != 0.:\n",
    "                points.append([i, j, horizon[i, j]])\n",
    "            else:\n",
    "                wanted.append([i, j])\n",
    "    \n",
    "    points = np.array(points)\n",
    "    zs2 = scipy.interpolate.griddata(points[:, 0:2], points[:, 2], wanted, method=\"cubic\")\n",
    "    for p, val in zip(wanted, zs2):\n",
    "        horizon[p[0], p[1]] = val\n",
    "    \n",
    "    return horizon\n",
    "\n",
    "def plot_section_horizon_and_well(ax, amplitude, horizon, twt, inline=38, well_pos=276//2):\n",
    "    hrz_idx = [np.abs(twt-val).argmin() for val in horizon[inline, :]]\n",
    "    \n",
    "    h_bin = np.zeros((amplitude.shape[0], amplitude.shape[2]))\n",
    "    for i, val in enumerate(hrz_idx):\n",
    "        h_bin[val, i] = 1\n",
    "\n",
    "    clip = abs(np.percentile(amplitude, 0.8))\n",
    "    ax.imshow(amplitude[:, inline], cmap=\"Greys\", vmin=-clip, vmax=clip)\n",
    "    ax.plot(range(len(hrz_idx)), hrz_idx, linewidth=5, color=\"black\")\n",
    "    ax.axvline(well_pos, color=\"red\", linewidth=5)\n",
    "\n",
    "def flatten_on_horizon(amplitude, horizon, twt, top_add=12, below_add=52):\n",
    "    traces = np.zeros((horizon.shape[0], horizon.shape[1], top_add+below_add))\n",
    "    for i in range(horizon.shape[0]):\n",
    "        hrz_idx = [np.abs(twt-val).argmin() for val in horizon[i, :]]\n",
    "        for j in range(horizon.shape[1]):\n",
    "            print(i, j)\n",
    "            traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "\n",
    "    return traces\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, hidden_size)\n",
    "        self.fc22 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(hidden_size, 128)\n",
    "        self.fc4 = nn.Linear(128, 256)\n",
    "        self.deconv1 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv1d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            if mu.is_cuda:\n",
    "                eps = eps.cuda()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "\n",
    "        out = self.relu(self.fc4(h3))\n",
    "\n",
    "        out = out.view(out.size(0), 32, 8)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "        out = self.relu(self.deconv2(out))\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.conv5(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "    \n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar, window_size=64):\n",
    "    criterion_mse = nn.MSELoss(size_average=False)\n",
    "    MSE = criterion_mse(recon_x.view(-1, 2, window_size), x.view(-1, 2, window_size))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD    \n",
    "    \n",
    "# Function to perform one epoch of training\n",
    "def train(epoch, model, optimizer, train_loader, cuda=False, log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, _ = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() * data.size(0) / len(train_loader.dataset)))\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# Function to perform evaluation of data on the model, used for testing\n",
    "def test(epoch, model, test_loader, cuda=False, log_interval=10):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, _ = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item() * data.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "# Function to forward_propagate a set of tensors and receive back latent variables and reconstructions\n",
    "def forward_all(model, all_loader, cuda=False):\n",
    "    model.eval()\n",
    "    reconstructions, latents = [], []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(all_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            reconstructions.append(recon_batch.cpu())\n",
    "            latents.append(z.cpu())\n",
    "    return torch.cat(reconstructions, 0), torch.cat(latents, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MY Classses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, field_name, inlines, xlines):\n",
    "        \n",
    "        # User input attributes\n",
    "        self.field_name = field_name\n",
    "        self.inlines = inlines\n",
    "        self.xlines = xlines\n",
    "\n",
    "        # near and far offset seismic data\n",
    "        self.near = None\n",
    "        self.far = None\n",
    "        self.twt = None\n",
    "        \n",
    "        # Dictionaries for multiple possible entries\n",
    "        self.horizons = {}\n",
    "        self.wells = {}\n",
    "        \n",
    "        \n",
    "    def add_segy(self, name, fname):\n",
    "        if name == 'near':\n",
    "            self.near, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        elif name == 'far':\n",
    "            self.far, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        else:\n",
    "            raise Exception('please specify if near or far data')\n",
    "            \n",
    "        if self.twt is None:\n",
    "            self.twt = twt\n",
    "        else: assert (self.twt == twt).all, \"This twt does not match the twt from the previous segy\"\n",
    "        \n",
    "    def add_horizon(self, horizon_name, fname):\n",
    "        self.horizons[horizon_name] = interpolate_horizon(load_horizon(fname, inlines=self.inlines, xlines=self.xlines))\n",
    "        \n",
    "    def add_well(self, well_id, well_i, well_x):\n",
    "        self.wells[well_id] = [well_i, well_x]\n",
    "        \n",
    "        \n",
    "class Processor:\n",
    "    def __init__(self, near=None, far=None, twt=None):\n",
    "        self.raw = [near, far]\n",
    "        self.twt = twt\n",
    "        self.out = None\n",
    "    \n",
    "    def flatten(self, data, horizon, top_add=12, below_add=52):\n",
    "        out = []\n",
    "        for amplitude in data:\n",
    "            traces = np.zeros((horizon.shape[0], horizon.shape[1], top_add+below_add))\n",
    "            for i in range(horizon.shape[0]):\n",
    "                hrz_idx = [np.abs(self.twt-val).argmin() for val in horizon[i, :]] \n",
    "                for j in range(horizon.shape[1]):\n",
    "                    traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "            out.append(traces)\n",
    "        return out\n",
    "    \n",
    "    def normalise(self, data, well_i=38, well_x=138):\n",
    "        out = []\n",
    "        for i in data:\n",
    "            well_variance = np.mean(np.std(i[well_i - 2:well_i + 1, well_x - 2:well_x + 1], 2))\n",
    "            i /= well_variance\n",
    "            out.append(i)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def to_2d(self, data):\n",
    "        return [i.reshape(-1, data[0].shape[-1]) for i in data]\n",
    "        \n",
    "    def average_neighbours(self, neighbours=10):\n",
    "        return 'not implemented yet'\n",
    "        \n",
    "    \n",
    "    def stack_traces(self, data):\n",
    "        return np.concatenate([i for i in data], 1)\n",
    "    \n",
    "    @property\n",
    "    def FF(self):\n",
    "        x_avo = self.out[0]\n",
    "        y_avo = self.out[1] - self.out[0]\n",
    "\n",
    "        lin_reg = LinearRegression(fit_intercept=False, normalize=False, copy_X=True, n_jobs=1)\n",
    "        lin_reg.fit(x_avo.reshape(-1, 1), y_avo.reshape(-1, 1))\n",
    "\n",
    "        print(\"Linear Regression coefficient: %1.2f\" % lin_reg.coef_[0, 0])\n",
    "        return y_avo - lin_reg.coef_ * x_avo\n",
    "    \n",
    "    def __call__(self, flatten=False, normalise=False, label='FF'):\n",
    "        self.out = copy.copy(self.raw)\n",
    "        \n",
    "        if flatten:\n",
    "            self.out = self.flatten(self.out, flatten[0], flatten[1], flatten[2])\n",
    "        if normalise:\n",
    "            self.out = self.normalise(self.out, normalise[0], normalise[1])\n",
    "        \n",
    "        # Flatten arrays from 3d to 2d\n",
    "        self.out = self.to_2d(self.out)\n",
    "        \n",
    "        # Pre-stacking calculation of FF\n",
    "        if label == 'FF':\n",
    "            self.label = self.FF\n",
    "            \n",
    "        # Stack the traces for output\n",
    "        self.out = self.stack_traces(self.out)\n",
    "        \n",
    "        return self.out, self.label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAgent:\n",
    "    def __init__(self, data):\n",
    "        self.input = data[0]\n",
    "        self.label = data[1]\n",
    "        print(\"ModelAgent initialised\")\n",
    "        \n",
    "    def plot_2d(self, data, label):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        sc = ax.scatter(data[:, 0], data[:, 1], s=2.0, c=np.min(label, 1))\n",
    "#         colorbar(sc)\n",
    "        return 1\n",
    "        \n",
    "    def plot_3d(self, data, feature):\n",
    "        return 'Not implemented'\n",
    "        \n",
    "        \n",
    "class UMAP(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    \n",
    "    def reduce(self, n_neighbors = 50, min_dist=0.001):\n",
    "        embedding_stack_ff = umap.UMAP(n_neighbors=n_neighbors,\n",
    "                      min_dist=min_dist,\n",
    "                      metric='correlation', \n",
    "                               verbose=False,\n",
    "#   this was in ASAP notebook: random_state=42).fit_transform(np.concatenate([stacked, FF.reshape(-1, 64)], 1))\n",
    "                            random_state=42).fit_transform(self.input)\n",
    "        \n",
    "        return embedding_stack_ff\n",
    "    \n",
    "    \n",
    "    \n",
    "class VAE_model(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32):\n",
    "        # split the concatenated input back into two arrays\n",
    "        X = torch.from_numpy(np.stack(np.split(self.input, 2, axis=1), 1)).float()\n",
    "        # Create a stacked representation and a zero tensor so we can use the standard Pytorch TensorDataset\n",
    "        y = torch.from_numpy(np.zeros((X.shape[0], 1))).float()\n",
    "\n",
    "        print('adf', X.shape)\n",
    "        \n",
    "        split = ShuffleSplit(n_splits=1, test_size=0.5)\n",
    "        for train_index, test_index in split.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        train_dset = TensorDataset(X_train, y_train)\n",
    "        test_dset = TensorDataset(X_test, y_test)\n",
    "        all_dset = TensorDataset(X, y)\n",
    "\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        self.all_loader = torch.utils.data.DataLoader(all_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "    def train_vae(self, cuda=False, epochs=30):\n",
    "        set_seed(42)  # Set the random seed\n",
    "        self.model = VAE(hidden_size=8)  # Inititalize the model\n",
    "\n",
    "        # use cuda if chosen\n",
    "        if cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        # Create a gradient descent optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "\n",
    "        # Store and plot losses\n",
    "        self.losses = []\n",
    "\n",
    "        # Start training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tl = train(epoch, self.model, optimizer, self.train_loader, cuda=False)  # Train model on train dataset\n",
    "            testl = test(epoch, self.model, self.test_loader, cuda=False)  # Validate model on test dataset\n",
    "            self.losses.append([tl, testl])\n",
    "        \n",
    "    def run_vae(self):\n",
    "        _, self.zs = forward_all(self.model, self.all_loader, cuda=False)\n",
    "        \n",
    "    def vae_umap(self):\n",
    "        transformer = umap.UMAP(n_neighbors=5,\n",
    "                                min_dist=0.001,\n",
    "                                metric='correlation', verbose=True).fit(self.zs.numpy())\n",
    "        embedding = transformer.transform(self.zs.numpy())\n",
    "        print('shape of zs', self.zs.shape)\n",
    "\n",
    "        # plot umap\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        sc = ax.scatter(embedding[::, 0], embedding[::, 1], s=2.0, c=np.min(self.label, 1)[::])\n",
    "        plt.show()\n",
    "    \n",
    "    def reduce(self, alpha=0.01):\n",
    "        self.create_dataloader()\n",
    "        self.train_vae()\n",
    "        self.run_vae()\n",
    "        self.vae_umap()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegyPY0.57:  readSegy : Trying to read ../data/3d_nearstack.sgy\n",
      "SegyPY0.57:  getSegyHeader : succesfully read ../data/3d_nearstack.sgy\n",
      "SegyPY0.57:  filesize=31438840\n",
      "SegyPY0.57:  bps=    4\n",
      "SegyPY0.57:  nd=7858810\n",
      "SegyPY0.57:  readSegyData : Reading segy data\n",
      "SegyPY0.57:  readSegyData : SEG-Y revision = 0\n",
      "SegyPY0.57:  readSegyData : DataSampleFormat=1(IBM Float)\n",
      "SegyPY0.57:   ns=250\n",
      "SegyPY0.57:  readSegyData : Finished reading segy data\n",
      "SegyPY0.57:  readSegy : Trying to read ../data/3d_farstack.sgy\n",
      "SegyPY0.57:  getSegyHeader : succesfully read ../data/3d_farstack.sgy\n",
      "SegyPY0.57:  filesize=31438840\n",
      "SegyPY0.57:  bps=    4\n",
      "SegyPY0.57:  nd=7858810\n",
      "SegyPY0.57:  readSegyData : Reading segy data\n",
      "SegyPY0.57:  readSegyData : SEG-Y revision = 0\n",
      "SegyPY0.57:  readSegyData : DataSampleFormat=1(IBM Float)\n",
      "SegyPY0.57:   ns=250\n",
      "SegyPY0.57:  readSegyData : Finished reading segy data\n"
     ]
    }
   ],
   "source": [
    "### Client loader\n",
    "Glitne = DataHolder(\"Glitne\", [1300, 1502, 2], [1500, 2002, 2])\n",
    "Glitne.add_segy('near', '../data/3d_nearstack.sgy');\n",
    "Glitne.add_segy('far', '../data/3d_farstack.sgy');\n",
    "Glitne.add_horizon('top_heimdal', '../data/Top_Heimdal_subset.txt')\n",
    "Glitne.add_well('well_1', 36, 276//2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression coefficient: -0.87\n",
      "Linear Regression coefficient: -0.86\n",
      "ModelAgent initialised\n"
     ]
    }
   ],
   "source": [
    "### Client data factory - would run diffetent instances of Processing with different operations\n",
    "# instance of processing creates options for many outputs\n",
    "Data = Processor(Glitne.near, Glitne.far, Glitne.twt)\n",
    "processing_a = Data([Glitne.horizons['top_heimdal'], 12,52], [10, 20])\n",
    "processing_b = Data([Glitne.horizons['top_heimdal'], 10,10], [10, 20])\n",
    "\n",
    "\n",
    "### Client model creator/run - run many different instances of the VAE with different parameters\n",
    "# an instance of a model is one model of dim reduction\n",
    "UMAP_a = UMAP(processing_a)\n",
    "# UMAP_a1 = UMAP_a.reduce(n_neighbors=10)\n",
    "# UMAP_a2 = UMAP_a.reduce(n_neighbors=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelAgent initialised\n",
      "adf torch.Size([25351, 2, 64])\n",
      "Train Epoch: 1 [0/12675 (0%)]\tLoss: 10.420815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [320/12675 (3%)]\tLoss: 9.607184\n",
      "Train Epoch: 1 [640/12675 (5%)]\tLoss: 9.590000\n",
      "Train Epoch: 1 [960/12675 (8%)]\tLoss: 9.452657\n",
      "Train Epoch: 1 [1280/12675 (10%)]\tLoss: 9.294279\n",
      "Train Epoch: 1 [1600/12675 (13%)]\tLoss: 9.683429\n",
      "Train Epoch: 1 [1920/12675 (15%)]\tLoss: 9.892874\n",
      "Train Epoch: 1 [2240/12675 (18%)]\tLoss: 10.079040\n",
      "Train Epoch: 1 [2560/12675 (20%)]\tLoss: 9.055381\n",
      "Train Epoch: 1 [2880/12675 (23%)]\tLoss: 9.133446\n",
      "Train Epoch: 1 [3200/12675 (25%)]\tLoss: 9.035822\n",
      "Train Epoch: 1 [3520/12675 (28%)]\tLoss: 9.195142\n",
      "Train Epoch: 1 [3840/12675 (30%)]\tLoss: 9.923202\n",
      "Train Epoch: 1 [4160/12675 (33%)]\tLoss: 9.074526\n",
      "Train Epoch: 1 [4480/12675 (35%)]\tLoss: 9.068188\n",
      "Train Epoch: 1 [4800/12675 (38%)]\tLoss: 9.543486\n",
      "Train Epoch: 1 [5120/12675 (40%)]\tLoss: 9.632841\n",
      "Train Epoch: 1 [5440/12675 (43%)]\tLoss: 8.731298\n",
      "Train Epoch: 1 [5760/12675 (45%)]\tLoss: 9.192215\n",
      "Train Epoch: 1 [6080/12675 (48%)]\tLoss: 9.302437\n",
      "Train Epoch: 1 [6400/12675 (50%)]\tLoss: 9.719193\n",
      "Train Epoch: 1 [6720/12675 (53%)]\tLoss: 9.523814\n",
      "Train Epoch: 1 [7040/12675 (55%)]\tLoss: 9.909175\n",
      "Train Epoch: 1 [7360/12675 (58%)]\tLoss: 9.421028\n",
      "Train Epoch: 1 [7680/12675 (60%)]\tLoss: 9.824598\n",
      "Train Epoch: 1 [8000/12675 (63%)]\tLoss: 10.037692\n",
      "Train Epoch: 1 [8320/12675 (65%)]\tLoss: 9.565886\n",
      "Train Epoch: 1 [8640/12675 (68%)]\tLoss: 8.743303\n",
      "Train Epoch: 1 [8960/12675 (71%)]\tLoss: 9.170114\n",
      "Train Epoch: 1 [9280/12675 (73%)]\tLoss: 9.133515\n",
      "Train Epoch: 1 [9600/12675 (76%)]\tLoss: 9.790311\n",
      "Train Epoch: 1 [9920/12675 (78%)]\tLoss: 9.825528\n",
      "Train Epoch: 1 [10240/12675 (81%)]\tLoss: 10.058831\n",
      "Train Epoch: 1 [10560/12675 (83%)]\tLoss: 9.732830\n",
      "Train Epoch: 1 [10880/12675 (86%)]\tLoss: 9.041885\n",
      "Train Epoch: 1 [11200/12675 (88%)]\tLoss: 9.351959\n",
      "Train Epoch: 1 [11520/12675 (91%)]\tLoss: 9.475513\n",
      "Train Epoch: 1 [11840/12675 (93%)]\tLoss: 8.901117\n",
      "Train Epoch: 1 [12160/12675 (96%)]\tLoss: 8.890409\n",
      "Train Epoch: 1 [12480/12675 (98%)]\tLoss: 9.704011\n",
      "====> Epoch: 1 Average loss: 3781.9324\n",
      "====> Test set loss: 3757.3897\n",
      "Train Epoch: 2 [0/12675 (0%)]\tLoss: 9.536845\n",
      "Train Epoch: 2 [320/12675 (3%)]\tLoss: 9.327922\n",
      "Train Epoch: 2 [640/12675 (5%)]\tLoss: 10.564625\n",
      "Train Epoch: 2 [960/12675 (8%)]\tLoss: 9.919625\n",
      "Train Epoch: 2 [1280/12675 (10%)]\tLoss: 9.224463\n",
      "Train Epoch: 2 [1600/12675 (13%)]\tLoss: 9.428692\n",
      "Train Epoch: 2 [1920/12675 (15%)]\tLoss: 9.672692\n",
      "Train Epoch: 2 [2240/12675 (18%)]\tLoss: 9.770873\n",
      "Train Epoch: 2 [2560/12675 (20%)]\tLoss: 8.945825\n",
      "Train Epoch: 2 [2880/12675 (23%)]\tLoss: 10.014028\n",
      "Train Epoch: 2 [3200/12675 (25%)]\tLoss: 8.796281\n",
      "Train Epoch: 2 [3520/12675 (28%)]\tLoss: 9.515261\n",
      "Train Epoch: 2 [3840/12675 (30%)]\tLoss: 9.364922\n",
      "Train Epoch: 2 [4160/12675 (33%)]\tLoss: 9.561739\n",
      "Train Epoch: 2 [4480/12675 (35%)]\tLoss: 10.045646\n",
      "Train Epoch: 2 [4800/12675 (38%)]\tLoss: 9.449060\n",
      "Train Epoch: 2 [5120/12675 (40%)]\tLoss: 8.682743\n",
      "Train Epoch: 2 [5440/12675 (43%)]\tLoss: 9.415813\n",
      "Train Epoch: 2 [5760/12675 (45%)]\tLoss: 9.022036\n",
      "Train Epoch: 2 [6080/12675 (48%)]\tLoss: 9.199189\n",
      "Train Epoch: 2 [6400/12675 (50%)]\tLoss: 9.938890\n",
      "Train Epoch: 2 [6720/12675 (53%)]\tLoss: 9.474611\n",
      "Train Epoch: 2 [7040/12675 (55%)]\tLoss: 9.191924\n",
      "Train Epoch: 2 [7360/12675 (58%)]\tLoss: 9.057665\n",
      "Train Epoch: 2 [7680/12675 (60%)]\tLoss: 9.153767\n",
      "Train Epoch: 2 [8000/12675 (63%)]\tLoss: 9.092955\n",
      "Train Epoch: 2 [8320/12675 (65%)]\tLoss: 9.650698\n",
      "Train Epoch: 2 [8640/12675 (68%)]\tLoss: 9.894436\n",
      "Train Epoch: 2 [8960/12675 (71%)]\tLoss: 9.563307\n",
      "Train Epoch: 2 [9280/12675 (73%)]\tLoss: 9.621648\n",
      "Train Epoch: 2 [9600/12675 (76%)]\tLoss: 9.565868\n",
      "Train Epoch: 2 [9920/12675 (78%)]\tLoss: 9.466855\n",
      "Train Epoch: 2 [10240/12675 (81%)]\tLoss: 9.742051\n",
      "Train Epoch: 2 [10560/12675 (83%)]\tLoss: 9.827772\n",
      "Train Epoch: 2 [10880/12675 (86%)]\tLoss: 9.193635\n",
      "Train Epoch: 2 [11200/12675 (88%)]\tLoss: 9.490756\n",
      "Train Epoch: 2 [11520/12675 (91%)]\tLoss: 9.530852\n",
      "Train Epoch: 2 [11840/12675 (93%)]\tLoss: 9.786694\n",
      "Train Epoch: 2 [12160/12675 (96%)]\tLoss: 10.175864\n",
      "Train Epoch: 2 [12480/12675 (98%)]\tLoss: 9.286223\n",
      "====> Epoch: 2 Average loss: 3771.2659\n",
      "====> Test set loss: 3751.0248\n",
      "Train Epoch: 3 [0/12675 (0%)]\tLoss: 9.512581\n",
      "Train Epoch: 3 [320/12675 (3%)]\tLoss: 9.199418\n",
      "Train Epoch: 3 [640/12675 (5%)]\tLoss: 9.919857\n",
      "Train Epoch: 3 [960/12675 (8%)]\tLoss: 8.884197\n",
      "Train Epoch: 3 [1280/12675 (10%)]\tLoss: 9.275488\n",
      "Train Epoch: 3 [1600/12675 (13%)]\tLoss: 9.385088\n",
      "Train Epoch: 3 [1920/12675 (15%)]\tLoss: 9.195621\n",
      "Train Epoch: 3 [2240/12675 (18%)]\tLoss: 9.784409\n",
      "Train Epoch: 3 [2560/12675 (20%)]\tLoss: 9.041068\n",
      "Train Epoch: 3 [2880/12675 (23%)]\tLoss: 9.455698\n",
      "Train Epoch: 3 [3200/12675 (25%)]\tLoss: 10.062046\n",
      "Train Epoch: 3 [3520/12675 (28%)]\tLoss: 8.756697\n",
      "Train Epoch: 3 [3840/12675 (30%)]\tLoss: 9.233030\n",
      "Train Epoch: 3 [4160/12675 (33%)]\tLoss: 9.314306\n",
      "Train Epoch: 3 [4480/12675 (35%)]\tLoss: 10.254715\n",
      "Train Epoch: 3 [4800/12675 (38%)]\tLoss: 9.619066\n",
      "Train Epoch: 3 [5120/12675 (40%)]\tLoss: 10.308399\n",
      "Train Epoch: 3 [5440/12675 (43%)]\tLoss: 9.009104\n",
      "Train Epoch: 3 [5760/12675 (45%)]\tLoss: 10.111789\n",
      "Train Epoch: 3 [6080/12675 (48%)]\tLoss: 10.035414\n",
      "Train Epoch: 3 [6400/12675 (50%)]\tLoss: 8.812708\n",
      "Train Epoch: 3 [6720/12675 (53%)]\tLoss: 9.469753\n",
      "Train Epoch: 3 [7040/12675 (55%)]\tLoss: 10.078295\n",
      "Train Epoch: 3 [7360/12675 (58%)]\tLoss: 9.070764\n",
      "Train Epoch: 3 [7680/12675 (60%)]\tLoss: 9.735219\n",
      "Train Epoch: 3 [8000/12675 (63%)]\tLoss: 9.310913\n",
      "Train Epoch: 3 [8320/12675 (65%)]\tLoss: 8.847935\n",
      "Train Epoch: 3 [8640/12675 (68%)]\tLoss: 9.419300\n",
      "Train Epoch: 3 [8960/12675 (71%)]\tLoss: 9.497200\n",
      "Train Epoch: 3 [9280/12675 (73%)]\tLoss: 9.651332\n",
      "Train Epoch: 3 [9600/12675 (76%)]\tLoss: 8.661885\n",
      "Train Epoch: 3 [9920/12675 (78%)]\tLoss: 9.771187\n",
      "Train Epoch: 3 [10240/12675 (81%)]\tLoss: 9.842525\n",
      "Train Epoch: 3 [10560/12675 (83%)]\tLoss: 9.381416\n",
      "Train Epoch: 3 [10880/12675 (86%)]\tLoss: 9.229671\n",
      "Train Epoch: 3 [11200/12675 (88%)]\tLoss: 9.756935\n",
      "Train Epoch: 3 [11520/12675 (91%)]\tLoss: 9.436349\n",
      "Train Epoch: 3 [11840/12675 (93%)]\tLoss: 10.394979\n",
      "Train Epoch: 3 [12160/12675 (96%)]\tLoss: 9.207295\n",
      "Train Epoch: 3 [12480/12675 (98%)]\tLoss: 9.557938\n",
      "====> Epoch: 3 Average loss: 3769.4520\n",
      "====> Test set loss: 3755.5015\n",
      "Train Epoch: 4 [0/12675 (0%)]\tLoss: 8.534935\n",
      "Train Epoch: 4 [320/12675 (3%)]\tLoss: 9.023263\n",
      "Train Epoch: 4 [640/12675 (5%)]\tLoss: 9.803446\n",
      "Train Epoch: 4 [960/12675 (8%)]\tLoss: 9.891662\n",
      "Train Epoch: 4 [1280/12675 (10%)]\tLoss: 9.856998\n",
      "Train Epoch: 4 [1600/12675 (13%)]\tLoss: 9.348449\n",
      "Train Epoch: 4 [1920/12675 (15%)]\tLoss: 9.747563\n",
      "Train Epoch: 4 [2240/12675 (18%)]\tLoss: 9.642009\n",
      "Train Epoch: 4 [2560/12675 (20%)]\tLoss: 9.768103\n",
      "Train Epoch: 4 [2880/12675 (23%)]\tLoss: 8.756018\n",
      "Train Epoch: 4 [3200/12675 (25%)]\tLoss: 8.690685\n",
      "Train Epoch: 4 [3520/12675 (28%)]\tLoss: 9.196865\n",
      "Train Epoch: 4 [3840/12675 (30%)]\tLoss: 9.540386\n",
      "Train Epoch: 4 [4160/12675 (33%)]\tLoss: 9.851320\n",
      "Train Epoch: 4 [4480/12675 (35%)]\tLoss: 9.688224\n",
      "Train Epoch: 4 [4800/12675 (38%)]\tLoss: 9.709997\n",
      "Train Epoch: 4 [5120/12675 (40%)]\tLoss: 9.060617\n",
      "Train Epoch: 4 [5440/12675 (43%)]\tLoss: 9.483374\n",
      "Train Epoch: 4 [5760/12675 (45%)]\tLoss: 9.684509\n",
      "Train Epoch: 4 [6080/12675 (48%)]\tLoss: 9.550369\n",
      "Train Epoch: 4 [6400/12675 (50%)]\tLoss: 9.144019\n",
      "Train Epoch: 4 [6720/12675 (53%)]\tLoss: 9.652933\n",
      "Train Epoch: 4 [7040/12675 (55%)]\tLoss: 10.134321\n",
      "Train Epoch: 4 [7360/12675 (58%)]\tLoss: 10.037417\n",
      "Train Epoch: 4 [7680/12675 (60%)]\tLoss: 9.655516\n",
      "Train Epoch: 4 [8000/12675 (63%)]\tLoss: 8.835949\n",
      "Train Epoch: 4 [8320/12675 (65%)]\tLoss: 9.761325\n",
      "Train Epoch: 4 [8640/12675 (68%)]\tLoss: 10.204859\n",
      "Train Epoch: 4 [8960/12675 (71%)]\tLoss: 9.617502\n",
      "Train Epoch: 4 [9280/12675 (73%)]\tLoss: 9.971059\n",
      "Train Epoch: 4 [9600/12675 (76%)]\tLoss: 9.510518\n",
      "Train Epoch: 4 [9920/12675 (78%)]\tLoss: 8.999126\n",
      "Train Epoch: 4 [10240/12675 (81%)]\tLoss: 9.972205\n",
      "Train Epoch: 4 [10560/12675 (83%)]\tLoss: 9.944186\n",
      "Train Epoch: 4 [10880/12675 (86%)]\tLoss: 8.625273\n",
      "Train Epoch: 4 [11200/12675 (88%)]\tLoss: 10.003924\n",
      "Train Epoch: 4 [11520/12675 (91%)]\tLoss: 8.810719\n",
      "Train Epoch: 4 [11840/12675 (93%)]\tLoss: 9.179925\n",
      "Train Epoch: 4 [12160/12675 (96%)]\tLoss: 9.944221\n",
      "Train Epoch: 4 [12480/12675 (98%)]\tLoss: 9.702503\n",
      "====> Epoch: 4 Average loss: 3769.5522\n",
      "====> Test set loss: 3754.5095\n",
      "Train Epoch: 5 [0/12675 (0%)]\tLoss: 9.817829\n",
      "Train Epoch: 5 [320/12675 (3%)]\tLoss: 9.917171\n",
      "Train Epoch: 5 [640/12675 (5%)]\tLoss: 9.960730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [960/12675 (8%)]\tLoss: 9.351937\n",
      "Train Epoch: 5 [1280/12675 (10%)]\tLoss: 9.434128\n",
      "Train Epoch: 5 [1600/12675 (13%)]\tLoss: 9.116040\n",
      "Train Epoch: 5 [1920/12675 (15%)]\tLoss: 10.240353\n",
      "Train Epoch: 5 [2240/12675 (18%)]\tLoss: 9.115788\n",
      "Train Epoch: 5 [2560/12675 (20%)]\tLoss: 9.371310\n",
      "Train Epoch: 5 [2880/12675 (23%)]\tLoss: 9.479669\n",
      "Train Epoch: 5 [3200/12675 (25%)]\tLoss: 9.713393\n",
      "Train Epoch: 5 [3520/12675 (28%)]\tLoss: 9.929147\n",
      "Train Epoch: 5 [3840/12675 (30%)]\tLoss: 8.918063\n",
      "Train Epoch: 5 [4160/12675 (33%)]\tLoss: 9.530819\n",
      "Train Epoch: 5 [4480/12675 (35%)]\tLoss: 9.632721\n",
      "Train Epoch: 5 [4800/12675 (38%)]\tLoss: 9.610268\n",
      "Train Epoch: 5 [5120/12675 (40%)]\tLoss: 9.452918\n",
      "Train Epoch: 5 [5440/12675 (43%)]\tLoss: 9.430332\n",
      "Train Epoch: 5 [5760/12675 (45%)]\tLoss: 8.995875\n",
      "Train Epoch: 5 [6080/12675 (48%)]\tLoss: 9.863892\n",
      "Train Epoch: 5 [6400/12675 (50%)]\tLoss: 9.323079\n",
      "Train Epoch: 5 [6720/12675 (53%)]\tLoss: 9.717957\n",
      "Train Epoch: 5 [7040/12675 (55%)]\tLoss: 9.603884\n",
      "Train Epoch: 5 [7360/12675 (58%)]\tLoss: 8.798929\n",
      "Train Epoch: 5 [7680/12675 (60%)]\tLoss: 9.495441\n",
      "Train Epoch: 5 [8000/12675 (63%)]\tLoss: 9.397283\n",
      "Train Epoch: 5 [8320/12675 (65%)]\tLoss: 9.435304\n",
      "Train Epoch: 5 [8640/12675 (68%)]\tLoss: 9.129638\n",
      "Train Epoch: 5 [8960/12675 (71%)]\tLoss: 9.862384\n",
      "Train Epoch: 5 [9280/12675 (73%)]\tLoss: 8.906413\n",
      "Train Epoch: 5 [9600/12675 (76%)]\tLoss: 9.213233\n",
      "Train Epoch: 5 [9920/12675 (78%)]\tLoss: 9.669914\n",
      "Train Epoch: 5 [10240/12675 (81%)]\tLoss: 9.808683\n",
      "Train Epoch: 5 [10560/12675 (83%)]\tLoss: 9.860256\n",
      "Train Epoch: 5 [10880/12675 (86%)]\tLoss: 8.826354\n",
      "Train Epoch: 5 [11200/12675 (88%)]\tLoss: 9.855398\n",
      "Train Epoch: 5 [11520/12675 (91%)]\tLoss: 9.932848\n",
      "Train Epoch: 5 [11840/12675 (93%)]\tLoss: 8.852374\n",
      "Train Epoch: 5 [12160/12675 (96%)]\tLoss: 9.803955\n",
      "Train Epoch: 5 [12480/12675 (98%)]\tLoss: 9.963718\n",
      "====> Epoch: 5 Average loss: 3767.9821\n",
      "====> Test set loss: 3750.8443\n",
      "Train Epoch: 6 [0/12675 (0%)]\tLoss: 9.077716\n",
      "Train Epoch: 6 [320/12675 (3%)]\tLoss: 9.613250\n",
      "Train Epoch: 6 [640/12675 (5%)]\tLoss: 8.898921\n",
      "Train Epoch: 6 [960/12675 (8%)]\tLoss: 8.665965\n",
      "Train Epoch: 6 [1280/12675 (10%)]\tLoss: 10.772500\n",
      "Train Epoch: 6 [1600/12675 (13%)]\tLoss: 10.572790\n",
      "Train Epoch: 6 [1920/12675 (15%)]\tLoss: 9.139079\n",
      "Train Epoch: 6 [2240/12675 (18%)]\tLoss: 9.750501\n",
      "Train Epoch: 6 [2560/12675 (20%)]\tLoss: 10.127050\n",
      "Train Epoch: 6 [2880/12675 (23%)]\tLoss: 9.975080\n",
      "Train Epoch: 6 [3200/12675 (25%)]\tLoss: 8.981932\n",
      "Train Epoch: 6 [3520/12675 (28%)]\tLoss: 9.693703\n",
      "Train Epoch: 6 [3840/12675 (30%)]\tLoss: 10.031253\n",
      "Train Epoch: 6 [4160/12675 (33%)]\tLoss: 9.710500\n",
      "Train Epoch: 6 [4480/12675 (35%)]\tLoss: 9.139573\n",
      "Train Epoch: 6 [4800/12675 (38%)]\tLoss: 8.933452\n",
      "Train Epoch: 6 [5120/12675 (40%)]\tLoss: 9.446903\n",
      "Train Epoch: 6 [5440/12675 (43%)]\tLoss: 9.185229\n",
      "Train Epoch: 6 [5760/12675 (45%)]\tLoss: 9.481456\n",
      "Train Epoch: 6 [6080/12675 (48%)]\tLoss: 8.963698\n",
      "Train Epoch: 6 [6400/12675 (50%)]\tLoss: 9.992108\n",
      "Train Epoch: 6 [6720/12675 (53%)]\tLoss: 9.366593\n",
      "Train Epoch: 6 [7040/12675 (55%)]\tLoss: 8.543256\n",
      "Train Epoch: 6 [7360/12675 (58%)]\tLoss: 10.073913\n",
      "Train Epoch: 6 [7680/12675 (60%)]\tLoss: 10.060028\n",
      "Train Epoch: 6 [8000/12675 (63%)]\tLoss: 9.915811\n",
      "Train Epoch: 6 [8320/12675 (65%)]\tLoss: 10.775201\n",
      "Train Epoch: 6 [8640/12675 (68%)]\tLoss: 9.647337\n",
      "Train Epoch: 6 [8960/12675 (71%)]\tLoss: 9.388029\n",
      "Train Epoch: 6 [9280/12675 (73%)]\tLoss: 9.049636\n",
      "Train Epoch: 6 [9600/12675 (76%)]\tLoss: 8.916528\n",
      "Train Epoch: 6 [9920/12675 (78%)]\tLoss: 9.975737\n",
      "Train Epoch: 6 [10240/12675 (81%)]\tLoss: 9.490958\n",
      "Train Epoch: 6 [10560/12675 (83%)]\tLoss: 9.922051\n",
      "Train Epoch: 6 [10880/12675 (86%)]\tLoss: 9.092104\n",
      "Train Epoch: 6 [11200/12675 (88%)]\tLoss: 9.385402\n",
      "Train Epoch: 6 [11520/12675 (91%)]\tLoss: 9.957842\n",
      "Train Epoch: 6 [11840/12675 (93%)]\tLoss: 9.304685\n",
      "Train Epoch: 6 [12160/12675 (96%)]\tLoss: 9.190589\n",
      "Train Epoch: 6 [12480/12675 (98%)]\tLoss: 9.606269\n",
      "====> Epoch: 6 Average loss: 3767.6517\n",
      "====> Test set loss: 3749.1565\n",
      "Train Epoch: 7 [0/12675 (0%)]\tLoss: 8.537518\n",
      "Train Epoch: 7 [320/12675 (3%)]\tLoss: 9.484305\n",
      "Train Epoch: 7 [640/12675 (5%)]\tLoss: 9.015195\n",
      "Train Epoch: 7 [960/12675 (8%)]\tLoss: 9.502279\n",
      "Train Epoch: 7 [1280/12675 (10%)]\tLoss: 10.021550\n",
      "Train Epoch: 7 [1600/12675 (13%)]\tLoss: 8.799750\n",
      "Train Epoch: 7 [1920/12675 (15%)]\tLoss: 10.195880\n",
      "Train Epoch: 7 [2240/12675 (18%)]\tLoss: 10.218096\n",
      "Train Epoch: 7 [2560/12675 (20%)]\tLoss: 9.630435\n",
      "Train Epoch: 7 [2880/12675 (23%)]\tLoss: 8.970820\n",
      "Train Epoch: 7 [3200/12675 (25%)]\tLoss: 10.015040\n",
      "Train Epoch: 7 [3520/12675 (28%)]\tLoss: 9.214417\n",
      "Train Epoch: 7 [3840/12675 (30%)]\tLoss: 9.975635\n",
      "Train Epoch: 7 [4160/12675 (33%)]\tLoss: 9.521068\n",
      "Train Epoch: 7 [4480/12675 (35%)]\tLoss: 8.915349\n",
      "Train Epoch: 7 [4800/12675 (38%)]\tLoss: 10.477087\n",
      "Train Epoch: 7 [5120/12675 (40%)]\tLoss: 8.828961\n",
      "Train Epoch: 7 [5440/12675 (43%)]\tLoss: 9.842077\n",
      "Train Epoch: 7 [5760/12675 (45%)]\tLoss: 9.360826\n",
      "Train Epoch: 7 [6080/12675 (48%)]\tLoss: 9.589098\n",
      "Train Epoch: 7 [6400/12675 (50%)]\tLoss: 9.725932\n",
      "Train Epoch: 7 [6720/12675 (53%)]\tLoss: 8.997161\n",
      "Train Epoch: 7 [7040/12675 (55%)]\tLoss: 9.537754\n",
      "Train Epoch: 7 [7360/12675 (58%)]\tLoss: 10.070147\n",
      "Train Epoch: 7 [7680/12675 (60%)]\tLoss: 9.848232\n",
      "Train Epoch: 7 [8000/12675 (63%)]\tLoss: 9.009467\n",
      "Train Epoch: 7 [8320/12675 (65%)]\tLoss: 9.677278\n",
      "Train Epoch: 7 [8640/12675 (68%)]\tLoss: 9.556423\n",
      "Train Epoch: 7 [8960/12675 (71%)]\tLoss: 8.736860\n",
      "Train Epoch: 7 [9280/12675 (73%)]\tLoss: 9.422853\n",
      "Train Epoch: 7 [9600/12675 (76%)]\tLoss: 9.437610\n",
      "Train Epoch: 7 [9920/12675 (78%)]\tLoss: 9.612488\n",
      "Train Epoch: 7 [10240/12675 (81%)]\tLoss: 10.054295\n",
      "Train Epoch: 7 [10560/12675 (83%)]\tLoss: 9.832428\n",
      "Train Epoch: 7 [10880/12675 (86%)]\tLoss: 9.014247\n",
      "Train Epoch: 7 [11200/12675 (88%)]\tLoss: 8.920904\n",
      "Train Epoch: 7 [11520/12675 (91%)]\tLoss: 9.118095\n",
      "Train Epoch: 7 [11840/12675 (93%)]\tLoss: 10.581127\n",
      "Train Epoch: 7 [12160/12675 (96%)]\tLoss: 9.167579\n",
      "Train Epoch: 7 [12480/12675 (98%)]\tLoss: 10.175689\n",
      "====> Epoch: 7 Average loss: 3770.3856\n",
      "====> Test set loss: 3756.4311\n",
      "Train Epoch: 8 [0/12675 (0%)]\tLoss: 9.791554\n",
      "Train Epoch: 8 [320/12675 (3%)]\tLoss: 9.493571\n",
      "Train Epoch: 8 [640/12675 (5%)]\tLoss: 9.475603\n",
      "Train Epoch: 8 [960/12675 (8%)]\tLoss: 9.527973\n",
      "Train Epoch: 8 [1280/12675 (10%)]\tLoss: 9.912075\n",
      "Train Epoch: 8 [1600/12675 (13%)]\tLoss: 9.122619\n",
      "Train Epoch: 8 [1920/12675 (15%)]\tLoss: 9.698179\n",
      "Train Epoch: 8 [2240/12675 (18%)]\tLoss: 9.796868\n",
      "Train Epoch: 8 [2560/12675 (20%)]\tLoss: 9.473266\n",
      "Train Epoch: 8 [2880/12675 (23%)]\tLoss: 9.023754\n",
      "Train Epoch: 8 [3200/12675 (25%)]\tLoss: 8.765444\n",
      "Train Epoch: 8 [3520/12675 (28%)]\tLoss: 8.743900\n",
      "Train Epoch: 8 [3840/12675 (30%)]\tLoss: 9.654973\n",
      "Train Epoch: 8 [4160/12675 (33%)]\tLoss: 9.764118\n",
      "Train Epoch: 8 [4480/12675 (35%)]\tLoss: 10.051580\n",
      "Train Epoch: 8 [4800/12675 (38%)]\tLoss: 9.571644\n",
      "Train Epoch: 8 [5120/12675 (40%)]\tLoss: 9.559138\n",
      "Train Epoch: 8 [5440/12675 (43%)]\tLoss: 10.088493\n",
      "Train Epoch: 8 [5760/12675 (45%)]\tLoss: 9.891159\n",
      "Train Epoch: 8 [6080/12675 (48%)]\tLoss: 9.870205\n",
      "Train Epoch: 8 [6400/12675 (50%)]\tLoss: 9.866678\n",
      "Train Epoch: 8 [6720/12675 (53%)]\tLoss: 9.679192\n",
      "Train Epoch: 8 [7040/12675 (55%)]\tLoss: 8.974081\n",
      "Train Epoch: 8 [7360/12675 (58%)]\tLoss: 9.909480\n",
      "Train Epoch: 8 [7680/12675 (60%)]\tLoss: 9.783872\n",
      "Train Epoch: 8 [8000/12675 (63%)]\tLoss: 9.074529\n",
      "Train Epoch: 8 [8320/12675 (65%)]\tLoss: 9.813635\n",
      "Train Epoch: 8 [8640/12675 (68%)]\tLoss: 10.251137\n",
      "Train Epoch: 8 [8960/12675 (71%)]\tLoss: 9.144023\n",
      "Train Epoch: 8 [9280/12675 (73%)]\tLoss: 10.104475\n",
      "Train Epoch: 8 [9600/12675 (76%)]\tLoss: 9.605680\n",
      "Train Epoch: 8 [9920/12675 (78%)]\tLoss: 10.569697\n",
      "Train Epoch: 8 [10240/12675 (81%)]\tLoss: 8.889408\n",
      "Train Epoch: 8 [10560/12675 (83%)]\tLoss: 9.165773\n",
      "Train Epoch: 8 [10880/12675 (86%)]\tLoss: 9.529250\n",
      "Train Epoch: 8 [11200/12675 (88%)]\tLoss: 9.456520\n",
      "Train Epoch: 8 [11520/12675 (91%)]\tLoss: 10.043029\n",
      "Train Epoch: 8 [11840/12675 (93%)]\tLoss: 8.714156\n",
      "Train Epoch: 8 [12160/12675 (96%)]\tLoss: 9.776969\n",
      "Train Epoch: 8 [12480/12675 (98%)]\tLoss: 8.742689\n",
      "====> Epoch: 8 Average loss: 3769.5828\n",
      "====> Test set loss: 3753.6758\n",
      "Train Epoch: 9 [0/12675 (0%)]\tLoss: 9.290905\n",
      "Train Epoch: 9 [320/12675 (3%)]\tLoss: 10.148599\n",
      "Train Epoch: 9 [640/12675 (5%)]\tLoss: 9.533116\n",
      "Train Epoch: 9 [960/12675 (8%)]\tLoss: 9.459643\n",
      "Train Epoch: 9 [1280/12675 (10%)]\tLoss: 9.030973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [1600/12675 (13%)]\tLoss: 9.273346\n",
      "Train Epoch: 9 [1920/12675 (15%)]\tLoss: 9.523267\n",
      "Train Epoch: 9 [2240/12675 (18%)]\tLoss: 9.516146\n",
      "Train Epoch: 9 [2560/12675 (20%)]\tLoss: 10.225354\n",
      "Train Epoch: 9 [2880/12675 (23%)]\tLoss: 9.272872\n",
      "Train Epoch: 9 [3200/12675 (25%)]\tLoss: 9.204021\n",
      "Train Epoch: 9 [3520/12675 (28%)]\tLoss: 10.155710\n",
      "Train Epoch: 9 [3840/12675 (30%)]\tLoss: 9.380068\n",
      "Train Epoch: 9 [4160/12675 (33%)]\tLoss: 8.651577\n",
      "Train Epoch: 9 [4480/12675 (35%)]\tLoss: 8.824230\n",
      "Train Epoch: 9 [4800/12675 (38%)]\tLoss: 9.434975\n",
      "Train Epoch: 9 [5120/12675 (40%)]\tLoss: 10.334941\n",
      "Train Epoch: 9 [5440/12675 (43%)]\tLoss: 9.084042\n",
      "Train Epoch: 9 [5760/12675 (45%)]\tLoss: 9.491182\n",
      "Train Epoch: 9 [6080/12675 (48%)]\tLoss: 8.515174\n",
      "Train Epoch: 9 [6400/12675 (50%)]\tLoss: 9.156331\n",
      "Train Epoch: 9 [6720/12675 (53%)]\tLoss: 9.328277\n",
      "Train Epoch: 9 [7040/12675 (55%)]\tLoss: 9.736135\n",
      "Train Epoch: 9 [7360/12675 (58%)]\tLoss: 9.513989\n",
      "Train Epoch: 9 [7680/12675 (60%)]\tLoss: 9.043984\n",
      "Train Epoch: 9 [8000/12675 (63%)]\tLoss: 9.707931\n",
      "Train Epoch: 9 [8320/12675 (65%)]\tLoss: 8.927622\n",
      "Train Epoch: 9 [8640/12675 (68%)]\tLoss: 8.865304\n",
      "Train Epoch: 9 [8960/12675 (71%)]\tLoss: 9.551666\n",
      "Train Epoch: 9 [9280/12675 (73%)]\tLoss: 9.035693\n",
      "Train Epoch: 9 [9600/12675 (76%)]\tLoss: 9.872083\n",
      "Train Epoch: 9 [9920/12675 (78%)]\tLoss: 9.168620\n",
      "Train Epoch: 9 [10240/12675 (81%)]\tLoss: 9.575070\n",
      "Train Epoch: 9 [10560/12675 (83%)]\tLoss: 10.419470\n",
      "Train Epoch: 9 [10880/12675 (86%)]\tLoss: 9.596550\n",
      "Train Epoch: 9 [11200/12675 (88%)]\tLoss: 10.435981\n",
      "Train Epoch: 9 [11520/12675 (91%)]\tLoss: 9.966411\n",
      "Train Epoch: 9 [11840/12675 (93%)]\tLoss: 10.167884\n",
      "Train Epoch: 9 [12160/12675 (96%)]\tLoss: 9.414263\n",
      "Train Epoch: 9 [12480/12675 (98%)]\tLoss: 9.686974\n",
      "====> Epoch: 9 Average loss: 3768.7679\n",
      "====> Test set loss: 3749.7889\n",
      "Train Epoch: 10 [0/12675 (0%)]\tLoss: 9.561487\n",
      "Train Epoch: 10 [320/12675 (3%)]\tLoss: 10.161927\n",
      "Train Epoch: 10 [640/12675 (5%)]\tLoss: 10.007240\n",
      "Train Epoch: 10 [960/12675 (8%)]\tLoss: 9.797232\n",
      "Train Epoch: 10 [1280/12675 (10%)]\tLoss: 10.321571\n",
      "Train Epoch: 10 [1600/12675 (13%)]\tLoss: 9.663760\n",
      "Train Epoch: 10 [1920/12675 (15%)]\tLoss: 9.129545\n",
      "Train Epoch: 10 [2240/12675 (18%)]\tLoss: 9.872551\n",
      "Train Epoch: 10 [2560/12675 (20%)]\tLoss: 9.860380\n",
      "Train Epoch: 10 [2880/12675 (23%)]\tLoss: 9.988744\n",
      "Train Epoch: 10 [3200/12675 (25%)]\tLoss: 9.441787\n",
      "Train Epoch: 10 [3520/12675 (28%)]\tLoss: 9.180382\n",
      "Train Epoch: 10 [3840/12675 (30%)]\tLoss: 9.280224\n",
      "Train Epoch: 10 [4160/12675 (33%)]\tLoss: 9.079207\n",
      "Train Epoch: 10 [4480/12675 (35%)]\tLoss: 9.797128\n",
      "Train Epoch: 10 [4800/12675 (38%)]\tLoss: 8.512753\n",
      "Train Epoch: 10 [5120/12675 (40%)]\tLoss: 9.868198\n",
      "Train Epoch: 10 [5440/12675 (43%)]\tLoss: 9.796561\n",
      "Train Epoch: 10 [5760/12675 (45%)]\tLoss: 9.839892\n",
      "Train Epoch: 10 [6080/12675 (48%)]\tLoss: 10.250444\n",
      "Train Epoch: 10 [6400/12675 (50%)]\tLoss: 11.028796\n",
      "Train Epoch: 10 [6720/12675 (53%)]\tLoss: 9.547282\n",
      "Train Epoch: 10 [7040/12675 (55%)]\tLoss: 8.921731\n",
      "Train Epoch: 10 [7360/12675 (58%)]\tLoss: 8.612939\n",
      "Train Epoch: 10 [7680/12675 (60%)]\tLoss: 9.423021\n",
      "Train Epoch: 10 [8000/12675 (63%)]\tLoss: 10.035642\n",
      "Train Epoch: 10 [8320/12675 (65%)]\tLoss: 10.208982\n",
      "Train Epoch: 10 [8640/12675 (68%)]\tLoss: 9.340111\n",
      "Train Epoch: 10 [8960/12675 (71%)]\tLoss: 8.973737\n",
      "Train Epoch: 10 [9280/12675 (73%)]\tLoss: 9.395373\n",
      "Train Epoch: 10 [9600/12675 (76%)]\tLoss: 9.258045\n",
      "Train Epoch: 10 [9920/12675 (78%)]\tLoss: 10.102538\n",
      "Train Epoch: 10 [10240/12675 (81%)]\tLoss: 10.239275\n",
      "Train Epoch: 10 [10560/12675 (83%)]\tLoss: 9.137686\n",
      "Train Epoch: 10 [10880/12675 (86%)]\tLoss: 9.084196\n",
      "Train Epoch: 10 [11200/12675 (88%)]\tLoss: 10.004771\n",
      "Train Epoch: 10 [11520/12675 (91%)]\tLoss: 9.875107\n",
      "Train Epoch: 10 [11840/12675 (93%)]\tLoss: 9.632144\n",
      "Train Epoch: 10 [12160/12675 (96%)]\tLoss: 9.101301\n",
      "Train Epoch: 10 [12480/12675 (98%)]\tLoss: 8.790901\n",
      "====> Epoch: 10 Average loss: 3769.9927\n",
      "====> Test set loss: 3753.8091\n",
      "Train Epoch: 11 [0/12675 (0%)]\tLoss: 9.389152\n",
      "Train Epoch: 11 [320/12675 (3%)]\tLoss: 8.820168\n",
      "Train Epoch: 11 [640/12675 (5%)]\tLoss: 9.699302\n",
      "Train Epoch: 11 [960/12675 (8%)]\tLoss: 9.358481\n",
      "Train Epoch: 11 [1280/12675 (10%)]\tLoss: 9.727038\n",
      "Train Epoch: 11 [1600/12675 (13%)]\tLoss: 10.068082\n",
      "Train Epoch: 11 [1920/12675 (15%)]\tLoss: 9.920328\n",
      "Train Epoch: 11 [2240/12675 (18%)]\tLoss: 9.483159\n",
      "Train Epoch: 11 [2560/12675 (20%)]\tLoss: 9.277475\n",
      "Train Epoch: 11 [2880/12675 (23%)]\tLoss: 9.405591\n",
      "Train Epoch: 11 [3200/12675 (25%)]\tLoss: 9.358124\n",
      "Train Epoch: 11 [3520/12675 (28%)]\tLoss: 9.212021\n",
      "Train Epoch: 11 [3840/12675 (30%)]\tLoss: 9.596784\n",
      "Train Epoch: 11 [4160/12675 (33%)]\tLoss: 9.611422\n",
      "Train Epoch: 11 [4480/12675 (35%)]\tLoss: 8.918935\n",
      "Train Epoch: 11 [4800/12675 (38%)]\tLoss: 9.212647\n",
      "Train Epoch: 11 [5120/12675 (40%)]\tLoss: 8.836190\n",
      "Train Epoch: 11 [5440/12675 (43%)]\tLoss: 9.246777\n",
      "Train Epoch: 11 [5760/12675 (45%)]\tLoss: 9.597576\n",
      "Train Epoch: 11 [6080/12675 (48%)]\tLoss: 9.737783\n",
      "Train Epoch: 11 [6400/12675 (50%)]\tLoss: 10.029320\n",
      "Train Epoch: 11 [6720/12675 (53%)]\tLoss: 9.319180\n",
      "Train Epoch: 11 [7040/12675 (55%)]\tLoss: 9.144731\n",
      "Train Epoch: 11 [7360/12675 (58%)]\tLoss: 9.995798\n",
      "Train Epoch: 11 [7680/12675 (60%)]\tLoss: 9.413738\n",
      "Train Epoch: 11 [8000/12675 (63%)]\tLoss: 9.470425\n",
      "Train Epoch: 11 [8320/12675 (65%)]\tLoss: 9.609757\n",
      "Train Epoch: 11 [8640/12675 (68%)]\tLoss: 9.627591\n",
      "Train Epoch: 11 [8960/12675 (71%)]\tLoss: 9.682893\n",
      "Train Epoch: 11 [9280/12675 (73%)]\tLoss: 9.528185\n",
      "Train Epoch: 11 [9600/12675 (76%)]\tLoss: 9.685304\n",
      "Train Epoch: 11 [9920/12675 (78%)]\tLoss: 10.382780\n",
      "Train Epoch: 11 [10240/12675 (81%)]\tLoss: 8.799962\n",
      "Train Epoch: 11 [10560/12675 (83%)]\tLoss: 9.063989\n",
      "Train Epoch: 11 [10880/12675 (86%)]\tLoss: 9.741021\n",
      "Train Epoch: 11 [11200/12675 (88%)]\tLoss: 9.982681\n",
      "Train Epoch: 11 [11520/12675 (91%)]\tLoss: 9.135256\n",
      "Train Epoch: 11 [11840/12675 (93%)]\tLoss: 9.038185\n",
      "Train Epoch: 11 [12160/12675 (96%)]\tLoss: 9.260031\n",
      "Train Epoch: 11 [12480/12675 (98%)]\tLoss: 8.905694\n",
      "====> Epoch: 11 Average loss: 3768.4784\n",
      "====> Test set loss: 3753.3128\n",
      "Train Epoch: 12 [0/12675 (0%)]\tLoss: 9.715289\n",
      "Train Epoch: 12 [320/12675 (3%)]\tLoss: 8.871903\n",
      "Train Epoch: 12 [640/12675 (5%)]\tLoss: 9.330475\n",
      "Train Epoch: 12 [960/12675 (8%)]\tLoss: 8.941440\n",
      "Train Epoch: 12 [1280/12675 (10%)]\tLoss: 9.833013\n",
      "Train Epoch: 12 [1600/12675 (13%)]\tLoss: 9.535766\n",
      "Train Epoch: 12 [1920/12675 (15%)]\tLoss: 9.849638\n",
      "Train Epoch: 12 [2240/12675 (18%)]\tLoss: 9.642087\n",
      "Train Epoch: 12 [2560/12675 (20%)]\tLoss: 9.599054\n",
      "Train Epoch: 12 [2880/12675 (23%)]\tLoss: 10.320214\n",
      "Train Epoch: 12 [3200/12675 (25%)]\tLoss: 9.165944\n",
      "Train Epoch: 12 [3520/12675 (28%)]\tLoss: 9.134262\n",
      "Train Epoch: 12 [3840/12675 (30%)]\tLoss: 9.490682\n",
      "Train Epoch: 12 [4160/12675 (33%)]\tLoss: 9.464765\n",
      "Train Epoch: 12 [4480/12675 (35%)]\tLoss: 8.958024\n",
      "Train Epoch: 12 [4800/12675 (38%)]\tLoss: 9.803374\n",
      "Train Epoch: 12 [5120/12675 (40%)]\tLoss: 10.345113\n",
      "Train Epoch: 12 [5440/12675 (43%)]\tLoss: 9.811478\n",
      "Train Epoch: 12 [5760/12675 (45%)]\tLoss: 9.510287\n",
      "Train Epoch: 12 [6080/12675 (48%)]\tLoss: 10.803301\n",
      "Train Epoch: 12 [6400/12675 (50%)]\tLoss: 9.679102\n",
      "Train Epoch: 12 [6720/12675 (53%)]\tLoss: 8.858630\n",
      "Train Epoch: 12 [7040/12675 (55%)]\tLoss: 9.025025\n",
      "Train Epoch: 12 [7360/12675 (58%)]\tLoss: 9.291041\n",
      "Train Epoch: 12 [7680/12675 (60%)]\tLoss: 8.861108\n",
      "Train Epoch: 12 [8000/12675 (63%)]\tLoss: 9.010693\n",
      "Train Epoch: 12 [8320/12675 (65%)]\tLoss: 10.075871\n",
      "Train Epoch: 12 [8640/12675 (68%)]\tLoss: 10.456821\n",
      "Train Epoch: 12 [8960/12675 (71%)]\tLoss: 8.908082\n",
      "Train Epoch: 12 [9280/12675 (73%)]\tLoss: 9.318159\n",
      "Train Epoch: 12 [9600/12675 (76%)]\tLoss: 9.228063\n",
      "Train Epoch: 12 [9920/12675 (78%)]\tLoss: 9.095359\n",
      "Train Epoch: 12 [10240/12675 (81%)]\tLoss: 10.126899\n",
      "Train Epoch: 12 [10560/12675 (83%)]\tLoss: 9.227541\n",
      "Train Epoch: 12 [10880/12675 (86%)]\tLoss: 9.428648\n",
      "Train Epoch: 12 [11200/12675 (88%)]\tLoss: 9.984436\n",
      "Train Epoch: 12 [11520/12675 (91%)]\tLoss: 9.614742\n",
      "Train Epoch: 12 [11840/12675 (93%)]\tLoss: 9.523362\n",
      "Train Epoch: 12 [12160/12675 (96%)]\tLoss: 9.859293\n",
      "Train Epoch: 12 [12480/12675 (98%)]\tLoss: 9.202914\n",
      "====> Epoch: 12 Average loss: 3768.3332\n",
      "====> Test set loss: 3753.1657\n",
      "Train Epoch: 13 [0/12675 (0%)]\tLoss: 9.556557\n",
      "Train Epoch: 13 [320/12675 (3%)]\tLoss: 8.857972\n",
      "Train Epoch: 13 [640/12675 (5%)]\tLoss: 9.764529\n",
      "Train Epoch: 13 [960/12675 (8%)]\tLoss: 10.356137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [1280/12675 (10%)]\tLoss: 9.598447\n",
      "Train Epoch: 13 [1600/12675 (13%)]\tLoss: 9.657479\n",
      "Train Epoch: 13 [1920/12675 (15%)]\tLoss: 9.609037\n",
      "Train Epoch: 13 [2240/12675 (18%)]\tLoss: 8.627039\n",
      "Train Epoch: 13 [2560/12675 (20%)]\tLoss: 9.664663\n",
      "Train Epoch: 13 [2880/12675 (23%)]\tLoss: 9.321458\n",
      "Train Epoch: 13 [3200/12675 (25%)]\tLoss: 9.162573\n",
      "Train Epoch: 13 [3520/12675 (28%)]\tLoss: 9.073551\n",
      "Train Epoch: 13 [3840/12675 (30%)]\tLoss: 9.469318\n",
      "Train Epoch: 13 [4160/12675 (33%)]\tLoss: 9.751798\n",
      "Train Epoch: 13 [4480/12675 (35%)]\tLoss: 9.393793\n",
      "Train Epoch: 13 [4800/12675 (38%)]\tLoss: 9.755821\n",
      "Train Epoch: 13 [5120/12675 (40%)]\tLoss: 9.412276\n",
      "Train Epoch: 13 [5440/12675 (43%)]\tLoss: 9.313409\n",
      "Train Epoch: 13 [5760/12675 (45%)]\tLoss: 8.879054\n",
      "Train Epoch: 13 [6080/12675 (48%)]\tLoss: 9.286445\n",
      "Train Epoch: 13 [6400/12675 (50%)]\tLoss: 9.339660\n",
      "Train Epoch: 13 [6720/12675 (53%)]\tLoss: 9.551836\n",
      "Train Epoch: 13 [7040/12675 (55%)]\tLoss: 9.948262\n",
      "Train Epoch: 13 [7360/12675 (58%)]\tLoss: 9.696554\n",
      "Train Epoch: 13 [7680/12675 (60%)]\tLoss: 9.203906\n",
      "Train Epoch: 13 [8000/12675 (63%)]\tLoss: 10.347543\n",
      "Train Epoch: 13 [8320/12675 (65%)]\tLoss: 9.848661\n",
      "Train Epoch: 13 [8640/12675 (68%)]\tLoss: 9.940309\n",
      "Train Epoch: 13 [8960/12675 (71%)]\tLoss: 8.267046\n",
      "Train Epoch: 13 [9280/12675 (73%)]\tLoss: 9.826490\n",
      "Train Epoch: 13 [9600/12675 (76%)]\tLoss: 9.973256\n",
      "Train Epoch: 13 [9920/12675 (78%)]\tLoss: 9.319308\n",
      "Train Epoch: 13 [10240/12675 (81%)]\tLoss: 10.231256\n",
      "Train Epoch: 13 [10560/12675 (83%)]\tLoss: 8.948068\n",
      "Train Epoch: 13 [10880/12675 (86%)]\tLoss: 10.132939\n",
      "Train Epoch: 13 [11200/12675 (88%)]\tLoss: 8.962782\n",
      "Train Epoch: 13 [11520/12675 (91%)]\tLoss: 9.601963\n",
      "Train Epoch: 13 [11840/12675 (93%)]\tLoss: 9.641201\n",
      "Train Epoch: 13 [12160/12675 (96%)]\tLoss: 9.624868\n",
      "Train Epoch: 13 [12480/12675 (98%)]\tLoss: 9.354251\n",
      "====> Epoch: 13 Average loss: 3767.9009\n",
      "====> Test set loss: 3750.6790\n",
      "Train Epoch: 14 [0/12675 (0%)]\tLoss: 10.454988\n",
      "Train Epoch: 14 [320/12675 (3%)]\tLoss: 9.427398\n",
      "Train Epoch: 14 [640/12675 (5%)]\tLoss: 9.508862\n",
      "Train Epoch: 14 [960/12675 (8%)]\tLoss: 9.235417\n",
      "Train Epoch: 14 [1280/12675 (10%)]\tLoss: 9.550831\n",
      "Train Epoch: 14 [1600/12675 (13%)]\tLoss: 9.219996\n",
      "Train Epoch: 14 [1920/12675 (15%)]\tLoss: 9.461129\n",
      "Train Epoch: 14 [2240/12675 (18%)]\tLoss: 9.470382\n",
      "Train Epoch: 14 [2560/12675 (20%)]\tLoss: 9.653074\n",
      "Train Epoch: 14 [2880/12675 (23%)]\tLoss: 9.124697\n",
      "Train Epoch: 14 [3200/12675 (25%)]\tLoss: 10.599830\n",
      "Train Epoch: 14 [3520/12675 (28%)]\tLoss: 9.493310\n",
      "Train Epoch: 14 [3840/12675 (30%)]\tLoss: 10.188023\n",
      "Train Epoch: 14 [4160/12675 (33%)]\tLoss: 9.930868\n",
      "Train Epoch: 14 [4480/12675 (35%)]\tLoss: 9.667374\n",
      "Train Epoch: 14 [4800/12675 (38%)]\tLoss: 9.134108\n",
      "Train Epoch: 14 [5120/12675 (40%)]\tLoss: 9.127769\n",
      "Train Epoch: 14 [5440/12675 (43%)]\tLoss: 9.257215\n",
      "Train Epoch: 14 [5760/12675 (45%)]\tLoss: 9.280231\n",
      "Train Epoch: 14 [6080/12675 (48%)]\tLoss: 9.209159\n",
      "Train Epoch: 14 [6400/12675 (50%)]\tLoss: 9.774573\n",
      "Train Epoch: 14 [6720/12675 (53%)]\tLoss: 9.448632\n",
      "Train Epoch: 14 [7040/12675 (55%)]\tLoss: 9.421106\n",
      "Train Epoch: 14 [7360/12675 (58%)]\tLoss: 9.797345\n",
      "Train Epoch: 14 [7680/12675 (60%)]\tLoss: 9.661037\n",
      "Train Epoch: 14 [8000/12675 (63%)]\tLoss: 9.960863\n",
      "Train Epoch: 14 [8320/12675 (65%)]\tLoss: 10.053653\n",
      "Train Epoch: 14 [8640/12675 (68%)]\tLoss: 10.053572\n",
      "Train Epoch: 14 [8960/12675 (71%)]\tLoss: 8.860546\n",
      "Train Epoch: 14 [9280/12675 (73%)]\tLoss: 9.506641\n",
      "Train Epoch: 14 [9600/12675 (76%)]\tLoss: 9.676103\n",
      "Train Epoch: 14 [9920/12675 (78%)]\tLoss: 9.643473\n",
      "Train Epoch: 14 [10240/12675 (81%)]\tLoss: 9.468526\n",
      "Train Epoch: 14 [10560/12675 (83%)]\tLoss: 8.644451\n",
      "Train Epoch: 14 [10880/12675 (86%)]\tLoss: 9.224235\n",
      "Train Epoch: 14 [11200/12675 (88%)]\tLoss: 10.211469\n",
      "Train Epoch: 14 [11520/12675 (91%)]\tLoss: 9.806139\n",
      "Train Epoch: 14 [11840/12675 (93%)]\tLoss: 9.645619\n",
      "Train Epoch: 14 [12160/12675 (96%)]\tLoss: 9.951520\n",
      "Train Epoch: 14 [12480/12675 (98%)]\tLoss: 9.733415\n",
      "====> Epoch: 14 Average loss: 3768.0637\n",
      "====> Test set loss: 3750.5899\n",
      "Train Epoch: 15 [0/12675 (0%)]\tLoss: 9.033319\n",
      "Train Epoch: 15 [320/12675 (3%)]\tLoss: 9.302025\n",
      "Train Epoch: 15 [640/12675 (5%)]\tLoss: 9.187238\n",
      "Train Epoch: 15 [960/12675 (8%)]\tLoss: 9.722469\n",
      "Train Epoch: 15 [1280/12675 (10%)]\tLoss: 9.807194\n",
      "Train Epoch: 15 [1600/12675 (13%)]\tLoss: 9.853688\n",
      "Train Epoch: 15 [1920/12675 (15%)]\tLoss: 10.015307\n",
      "Train Epoch: 15 [2240/12675 (18%)]\tLoss: 9.495090\n",
      "Train Epoch: 15 [2560/12675 (20%)]\tLoss: 8.753661\n",
      "Train Epoch: 15 [2880/12675 (23%)]\tLoss: 9.846799\n",
      "Train Epoch: 15 [3200/12675 (25%)]\tLoss: 9.351716\n",
      "Train Epoch: 15 [3520/12675 (28%)]\tLoss: 10.000751\n",
      "Train Epoch: 15 [3840/12675 (30%)]\tLoss: 8.514962\n",
      "Train Epoch: 15 [4160/12675 (33%)]\tLoss: 9.089783\n",
      "Train Epoch: 15 [4480/12675 (35%)]\tLoss: 9.386498\n",
      "Train Epoch: 15 [4800/12675 (38%)]\tLoss: 10.192095\n",
      "Train Epoch: 15 [5120/12675 (40%)]\tLoss: 9.868847\n",
      "Train Epoch: 15 [5440/12675 (43%)]\tLoss: 9.596019\n",
      "Train Epoch: 15 [5760/12675 (45%)]\tLoss: 9.656718\n",
      "Train Epoch: 15 [6080/12675 (48%)]\tLoss: 9.034578\n",
      "Train Epoch: 15 [6400/12675 (50%)]\tLoss: 9.054644\n",
      "Train Epoch: 15 [6720/12675 (53%)]\tLoss: 9.705950\n",
      "Train Epoch: 15 [7040/12675 (55%)]\tLoss: 9.287429\n",
      "Train Epoch: 15 [7360/12675 (58%)]\tLoss: 8.651386\n",
      "Train Epoch: 15 [7680/12675 (60%)]\tLoss: 9.407558\n",
      "Train Epoch: 15 [8000/12675 (63%)]\tLoss: 8.680093\n",
      "Train Epoch: 15 [8320/12675 (65%)]\tLoss: 9.456350\n",
      "Train Epoch: 15 [8640/12675 (68%)]\tLoss: 8.779329\n",
      "Train Epoch: 15 [8960/12675 (71%)]\tLoss: 10.419255\n",
      "Train Epoch: 15 [9280/12675 (73%)]\tLoss: 10.103020\n",
      "Train Epoch: 15 [9600/12675 (76%)]\tLoss: 9.308141\n",
      "Train Epoch: 15 [9920/12675 (78%)]\tLoss: 9.541950\n",
      "Train Epoch: 15 [10240/12675 (81%)]\tLoss: 9.769020\n",
      "Train Epoch: 15 [10560/12675 (83%)]\tLoss: 10.198306\n",
      "Train Epoch: 15 [10880/12675 (86%)]\tLoss: 8.818844\n",
      "Train Epoch: 15 [11200/12675 (88%)]\tLoss: 9.249101\n",
      "Train Epoch: 15 [11520/12675 (91%)]\tLoss: 9.611569\n",
      "Train Epoch: 15 [11840/12675 (93%)]\tLoss: 9.984490\n",
      "Train Epoch: 15 [12160/12675 (96%)]\tLoss: 9.960566\n",
      "Train Epoch: 15 [12480/12675 (98%)]\tLoss: 9.684267\n",
      "====> Epoch: 15 Average loss: 3768.1631\n",
      "====> Test set loss: 3750.9284\n",
      "Train Epoch: 16 [0/12675 (0%)]\tLoss: 9.542559\n",
      "Train Epoch: 16 [320/12675 (3%)]\tLoss: 9.077302\n",
      "Train Epoch: 16 [640/12675 (5%)]\tLoss: 9.632163\n",
      "Train Epoch: 16 [960/12675 (8%)]\tLoss: 8.754175\n",
      "Train Epoch: 16 [1280/12675 (10%)]\tLoss: 9.454596\n",
      "Train Epoch: 16 [1600/12675 (13%)]\tLoss: 10.234339\n",
      "Train Epoch: 16 [1920/12675 (15%)]\tLoss: 8.728420\n",
      "Train Epoch: 16 [2240/12675 (18%)]\tLoss: 9.695746\n",
      "Train Epoch: 16 [2560/12675 (20%)]\tLoss: 9.921527\n",
      "Train Epoch: 16 [2880/12675 (23%)]\tLoss: 10.749209\n",
      "Train Epoch: 16 [3200/12675 (25%)]\tLoss: 9.584056\n"
     ]
    }
   ],
   "source": [
    "# give data once\n",
    "VAE_a = VAE_model(processing_a)\n",
    "\n",
    "# run with different params\n",
    "VAE_a.create_dataloader()\n",
    "VAE_a.train_vae()\n",
    "VAE_a.run_vae()\n",
    "VAE_a.vae_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
